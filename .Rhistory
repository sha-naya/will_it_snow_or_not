summarise( meanSL=mean(Sepal.Length),
sdSL=sd(Sepal.Length),
meanSW= mean(Sepal.Width),
sdSW= sd(Sepal.Width)) %>%
filter(meanSL==max(meanSL) | meanSW==max(meanSW))
library(tidyverse)
library(nycflights13)
flights2 <- flights %>%
select(year:day, hour, origin, dest, tailnum, carrier)
flights2
airlines
flights2 %>%
select(-origin, -dest) %>%
left_join(airlines, by = "carrier")
weather
names(flights2)
names(weather)
intersect(names(flights2), names(weather))
planes
intersect(names(flights2), names(planes))
flights2 %>%
left_join(planes, by = "tailnum") %>%  View()
flights2 %>%
left_join(planes, by = "tailnum", suffix=c("_left","_right")) %>%  View()
library(plotly)
flights3 <- flights2 %>%
group_by(dest) %>%
summarise(count = n())
flights3
flights4 <- flights3 %>%
inner_join(airports, c("dest" = "faa"))
flights4
geo <- list(
scope = 'usa',
projection = list(type = 'albers usa'),
showland = TRUE,
landcolor = toRGB("gray95"),
subunitcolor = toRGB("gray85"),
countrycolor = toRGB("gray85"),
countrywidth = 0.5,
subunitwidth = 0.5
)
geo
plot_geo(flights4, lat = ~lat, lon = ~lon) %>%
add_markers(text = ~paste(dest, name,  paste("Total:", count), sep = "<br />"),
color = ~count, symbol = I("square"), size = I(8), hoverinfo = "text"
) %>%
colorbar(title = "nycflights13 data") %>%
layout(title = 'NYC Flights Dataset<br />(Hover for airport)',
geo = geo)
set.seed(123)
df <- data.frame(Name = paste0("P", 1:5),
Age = sample(20:60, 5))
df
df %>%
inner_join(df, by= character(),
suffix=c("1","2")) %>%
filter(Age1 < Age2)
times100 <- read.csv('https://people.bu.edu/kalathur/datasets/times100.csv')
times100
set.seed(389)
library(sampling)
#a
sample1 <- srswor(12, nrow(times100))
times100[s != 0]
times100[sample1 != 0]
sample1
times100[sample1 != 0,]
#a
s <- srswor(12, nrow(times100))
sample1 <- times100[sample1 != 0,]
sample1
count(sample1$location)
distinct(sample1$location)
sample1$location
group_by(sample1$location)
mean(sample1$scores_overall)
#b
k <- ceiling(nrow(times100) / 12)
#b
k <- ceiling(nrow(times100) / 12)
r <- sample(k, 1)
seq(r, by = k, length = 12)
s2 <- seq(r, by = k, length = 12)
#a
s <- srswor(12, nrow(times100))
sample1 <- times100[s != 0,]
sample1
sample2 <- times100[s2 != 0,]
sample2
sample2 <- times100[s2,]
sample2
mean(sample2$scores_overall)
#b
k <- ceiling(nrow(times100) / 12)
r <- sample(k, 1)
s2 <- seq(r, by = k, length = 12)
sample2 <- times100[s2,]
sample2
sample2[1:11]
sample2[1:10]
sample2[1:11,]
mean(sample2[1:11,]$scores_overall)
#c
sample3 <- sampling::strata(times100, stratanames = c("location"),
size = rep(4, 12), method = "srswor",
description = TRUE)
#c
sample3 <- sampling::strata(times100, stratanames = c("location"),
size = rep(12, 4), method = "srswor",
description = TRUE)
#c
sample3 <- sampling::strata(times100, stratanames = c("location"),
size = rep(4, 12), method = "srswor",
description = TRUE)
aggregate(times100, by = c('location'), FUN = count)
aggregate(times100, by = list('location'), FUN = count)
aggregate(times100$location, by = list('location'), FUN = count)
unique(times100)
unique(times100$location)
times100 %>% group_by('location') %>% summarise(total_rows = n())
times100 %>% group_by(location) %>% summarise(total_rows = n())
sample1 %>% group_by(location) %>% summarise(total_rows = n())
max(times100 %>% group_by(location) %>% summarise(total_rows = n()))
max(times100 %>% group_by(location) %>% summarise(total_rows = n()),)
sample1 %>% group_by(location) %>% summarise(total_rows = n()) %>% arrange(total_rows)
sample1 %>% group_by(location) %>% summarise(total_rows = n()) %>% arrange(total_rows, desc)
sample1 %>% group_by(location) %>% summarise(total_rows = n())[-1,]
sample1 %>% group_by(location) %>% summarise(total_rows = n())[1,]
sample1 %>% group_by(location) %>% summarise(total_rows = n())
tail(sample1 %>% group_by(location) %>% summarise(total_rows = n()))
tail(sample1 %>% group_by(location) %>% summarise(total_rows = n()), 1)
times100 %>% group_by(location) %>% summarise(total_rows = n())
times100 %>% group_by(location) %>% summarise(total_rows = n()) %>% arrange(location)
times100 %>% group_by(location) %>% summarise(total_rows = n()) %>% arrange(location)$total_rows
prob1 <- times100 %>% group_by(location) %>% summarise(total_rows = n()) %>% arrange(location)
prob1$total_rows
prob2 <- prob1$total_rows
prob2
s2 <- UPsystematic(pik)
pik <- inclusionprobabilities(prob2, 12)
s2 <- UPsystematic(pik)
sample2 <- times100[s2,]
sample2
#c
sample3 <- sampling::strata(times100, stratanames = c("location"),
size = rep(4, 12), method = "srswor",
description = TRUE)
times100 %>% group_by(location) %>% summarise(total_rows = n())
times100 %>% group_by(location) %>% summarise(total_rows = n()) %>% arrange(total_rows)
tail(times100 %>% group_by(location) %>% summarise(total_rows = n()) %>% arrange(total_rows), 4)
#c
top4 <- tail(times100 %>% group_by(location) %>% summarise(total_rows = n()) %>% arrange(total_rows), 4)
top4_countries <- top4$location
subset1 <- filter(times100, location==top4_countries)
subset1
sample3 <- sampling::strata(times100, stratanames = c("location"),
size = rep(4, 12), method = "srswor",
description = TRUE)
sample3 <- sampling::strata(subset1, stratanames = c("location"),
size = rep(4, 12), method = "srswor",
description = TRUE)
sample3 <- sampling::strata(subset1, stratanames = c("location"),
size = rep(4, 3), method = "srswor",
description = TRUE)
sample3 <- sampling::strata(subset1, stratanames = c("location"),
size = rep(3, 4), method = "srswor",
description = TRUE)
sample3 <- sampling::strata(subset1, stratanames = c("location"),
size = rep(3), method = "srswor",
description = TRUE)
sample3 <- sampling::strata(subset1, stratanames = c("location"),
size = rep(3, 2), method = "srswor",
description = TRUE)
sample3 <- sampling::strata(subset1, stratanames = c("location"),
size = rep(3, 4), method = "srswor",
description = TRUE)
top4
subset1 <- filter(times100, location==top4_countries)
subset1
top4_countries
subset1 <- filter(times100, location=='Netherlands' | location=='Germany' | location=='United Kingdom' | location=='United States')
sample3 <- sampling::strata(subset1, stratanames = c("location"),
size = rep(3, 4), method = "srswor",
description = TRUE)
aggregate(times100$location, by = list('location'), FUN = count)
aggregate(sample3$location, by = list('location'), FUN = count)
mean(sample3$scores_overall)
#2
#a
continents <- read.csv('https://people.bu.edu/kalathur/datasets/continents.csv')
times100
continents
prob1 <- times100 %>%
group_by(location) %>%
summarise(total_rows = n(),
total_number_students=sum(stats_number_students),
avg = mean(stats_number_students)
) %>%
arrange(location)
prob1
prob1 <- times100 %>%
group_by(location) %>%
summarise(total_students=sum(stats_number_students),
avg_students = mean(stats_number_students)
) %>%
arrange(avg_students)
a <- times100 %>%
group_by(location) %>%
summarise(total_students=sum(stats_number_students),
avg_students = mean(stats_number_students)
) %>%
arrange(avg_students)
a
#b
times100 %>%
inner_join(continents)
#b
continents %>% mutate(location=continents$country)
continents
#b
new_continents <- continents %>% mutate(location=continents$country)
times100 %>%
inner_join(new_continents)
new_times100 <- times100 %>%
inner_join(new_continents)
b <- new_times100 %>%
group_by(continent) %>%
summarise(uni_count=n(),
max = max(stats_number_students),
min = min(stats_number_students),
country_count = count(unique(location))
)
b <- new_times100 %>%
group_by(continent) %>%
summarise(uni_count=n(),
max = max(stats_number_students),
min = min(stats_number_students),
country_count = n(unique(location))
)
b <- new_times100 %>%
group_by(continent) %>%
summarise(uni_count=n(),
max = max(stats_number_students),
min = min(stats_number_students),
country_count = nrow(unique(location))
)
b
#c
asia <- filter(new_times100, continent=="Asia")
asia
nrow(asia)
sum(asia$stats_number_students)
mean(asia$stats_number_students)
asia[asia$stats_number_students > mean(asia$stats_number_students),]
nrow(asia[asia$stats_number_students > mean(asia$stats_number_students),])
#a
s <- srswor(12, nrow(times100))
sample1 <- times100[s != 0,]
sample1
sample1 %>% group_by(location) %>% summarise(total_rows = n())
tail(sample1 %>% group_by(location) %>% summarise(total_rows = n()), 1)
mean(sample1$scores_overall)
reviews <- read.table("/Users/ayan/Downloads/aclImdb/train/urls_neg.txt")
reviews
neg_reviews <- read.table("/Users/ayan/Downloads/aclImdb/train/neg/0_3.txt")
neg_reviews
x <- c(16023,220,29645,25911,1740,1668,0,19,6,1,2461,1073,20,43,0,0,0,0,27,29,13,0,0,0,78899)
len(x)
length(x)
data <- read.table("/Users/ayan/Downloads/creditcard.csv")
data
data <- read.csv("/Users/ayan/Downloads/creditcard.csv")
data
head(data)
data <- read.csv("/Users/ayan/Downloads/weather.csv")
table(data)
table(data$WT09_ATTRIBUTES)
table(data$WT09)
View(data)
table(data$PGTM)
table(data$PGTM_ATTRIBUTES)
table(data$TAVG)
table(data$TAVG_ATTRIBUTES)
table(data$WT02)
table(data$WT03)
table(data$WT04)
table(data$AWND_ATTRIBUTES)
table(data$PGTM)
table(data$PGTM_ATTRIBUTES)
table(data$PRCP_ATTRIBUTES)
table(data$SNOW_ATTRIBUTES)
table(data$TAVG_ATTRIBUTES)
table(data$TMAX_ATTRIBUTES)
table(data$TMIN_ATTRIBUTES)
table(data$SNOW)
pnorm(q = 4.5393, mean = 5.454, sd = 0.5924)
pnorm(q = 57.22, mean = 55, sd = 0.74)
pnorm(q = 57.22, mean = 55, sd = 0.74) - pnorm(q = 52.78, mean = 55, sd = 0.74)
median(c(49, 95, 22, 51, 29, 40))
pnorm(q = 860, mean = 1172, sd = 318/sqrt(85))
pnorm(q = 278, mean = 258, sd = 10) - pnorm(q = 238, mean = 258, sd = 10)
qnorm(p = 0.03, mean = 0, sd = 1, lower.tail = FALSE)
mean(c(27, 89, 33, 99, 4, 59))
(273 - 292) / 7
pnorm(q = 301, mean = 280, sd = 5, lower.tail = FALSE)
#3
library(RWeka)
#3
install.packages("rJava", type = "source")
install.packages("RWeka", type = "source")
library(rJava)
sudo R CMD javareconf
CMD javareconf
#3
install.packages("rJava", type = "source")
#3
install.packages("rJava", type = "source")
Sys.setenv(JAVA_HOME="/Users/ayan/Library/Caches/Coursier/jvm/adopt@1.11.0-11/Contents/Home")
Sys.setenv(JAVA_HOME="/Users/ayan/Library/Caches/Coursier/jvm/adopt@1.11.0-11/Contents/Home")
library(rJava)
#3
install.packages("rJava", type = "source")
n <- 57
xbar <- 4.8
s <- 1.74
margin <- qt(0.975,df=n-1)*s/sqrt(n)
lowerinterval <- xbar - margin
lowerinterval
upperinterval <- xbar + margin
upperinterval
n <- 85
margin2 <- qt(0.975,df=n-1)*s/sqrt(n)
answer <- 100 - margin2
answer
n <- 85
margin2 <- qt(0.975,df=n-1)*s/sqrt(n)
71 + margin2
(92-94.9)/8
(94.9-92)/8
install.packages("asbio")
library(asbio)
one.sample.z(null.mu = 83, xbar = 84.93, sigma = 6, n=50, alternative = "two.sided", conf = 0.95)
one.sample.z(null.mu = 92, xbar = 94.9, sigma = 8, n=45, alternative = "two.sided", conf = 0.95)
one.sample.t(null.mu = 100, sd = 5, xbar = 104, n = 10)
qt(p = 0.81, df = 22, lower.tail = TRUE)
one.sample.t(null.mu = 96, sd = 8, xbar = 101.69, n = 16, alternative = "two.sided", conf = 0.975)
pt(4.45, 8)
lowerinterval
upperinterval
regression_line <- lm(c(20,32,22,26,30) ~ c(20,30,24,28,28))
anova(regression_line)
summary(regression_line)
qf(0.99, df1=1, df2=98)
qf(0.01, df1=1, df2=98)
qf(0.99, df1=1, df2=98)
pf(0.01, df=1, df2=98)
pf(0.99, df=1, df2=98)
qf(0.99, df1=1, df2=3)
# set working directory
setwd("/Users/ayan/Desktop/BU/Spring 2023/CS699_project")
# import data file
weather_data <- read.csv("weather_data.csv")
# these are the features mentioned above
features <- c(
'AWND',
'PRCP',
'PRCP_ATTRIBUTES',
'SNOW_ATTRIBUTES',
'TAVG',
'TMAX',
'TMAX_ATTRIBUTES',
'TMIN',
'WDF2',
'WDF5',
'WSF2',
'WSF5',
'WT01',
'WT02',
'WT03',
'WT04',
'WT05',
'WT06',
'WT08',
'WT09',
'weather_condition'
)
# we take only the columns/features we need
weather_data_subset <- weather_data[features]
# change categorical variables to numeric
weather_data_subset$PRCP_ATTRIBUTES <- factor(
weather_data_subset$PRCP_ATTRIBUTES,
levels = c(',,D,2400', ',,W,2400', 'T,,D,2400', 'T,,W,2400'),
labels = c(2, 4, 1, 3)
)
weather_data_subset$SNOW_ATTRIBUTES <- factor(
weather_data_subset$SNOW_ATTRIBUTES,
levels = c(',,D', ',,W', 'T,,D', 'T,,W'),
labels = c(2, 4, 1, 3)
)
weather_data_subset$TMAX_ATTRIBUTES <- factor(
weather_data_subset$TMAX_ATTRIBUTES,
levels = c(',,D', ',,W'),
labels = c(1, 2)
)
# fill NA values with 0
weather_data_subset$WT01[is.na(weather_data_subset$WT01)] <- 0
weather_data_subset$WT02[is.na(weather_data_subset$WT02)] <- 0
weather_data_subset$WT03[is.na(weather_data_subset$WT03)] <- 0
weather_data_subset$WT04[is.na(weather_data_subset$WT04)] <- 0
weather_data_subset$WT05[is.na(weather_data_subset$WT05)] <- 0
weather_data_subset$WT06[is.na(weather_data_subset$WT06)] <- 0
weather_data_subset$WT08[is.na(weather_data_subset$WT08)] <- 0
weather_data_subset$WT09[is.na(weather_data_subset$WT09)] <- 0
# scale numeric column to mean = 0 and sd = 1
weather_data_subset$AWND <- scale(weather_data_subset$AWND)
weather_data_subset$PRCP <- scale(weather_data_subset$PRCP)
weather_data_subset$TAVG <- scale(weather_data_subset$TAVG)
weather_data_subset$TMAX <- scale(weather_data_subset$TMAX)
weather_data_subset$TMIN <- scale(weather_data_subset$TMIN)
weather_data_subset$WDF2 <- scale(weather_data_subset$WDF2)
weather_data_subset$WDF5 <- scale(weather_data_subset$WDF5)
weather_data_subset$WSF2 <- scale(weather_data_subset$WSF2)
weather_data_subset$WSF5 <- scale(weather_data_subset$WSF5)
# feature selection #1
install.packages("party")
library(party)
cf1 <- cforest(weather_condition ~ . , data= weather_data_subset, control=cforest_unbiased(mtry=2,ntree=50))
sapply(weather_data_subset, class)
weather_data_subset$weather_condition <- as.factor(weather_data_subset$weather_condition)
cf1 <- cforest(weather_condition ~ . , data= weather_data_subset, control=cforest_unbiased(mtry=2,ntree=50))
varimpAUC(cf1)
install.packages("varImp")
library(varImp)
varimpAUC(cf1)
#2
control <- trainControl(method="repeatedcv", number=10, repeats=10, sampling="down")
#2
library(caret)
control <- trainControl(method="repeatedcv", number=10, repeats=10, sampling="down")
# train the model
model <- train(weather_condition~., data=weather_data_subset, method="lvq", preProcess="scale", trControl=control)
is.na(weather_data_subset)
colSums(is.na(weather_data_subset))
names(which(colSums(is.na(weather_data_subset))>0))
View(weather_data_subset)
weather_data_subset$WDF5[is.na(weather_data_subset$WDF5)] <- 0
weather_data_subset$WSF5[is.na(weather_data_subset$WSF5)] <- 0
# train the model
model <- train(weather_condition~., data=weather_data_subset, method="lvq", preProcess="scale", trControl=control)
#wdf5, wsf5
importance <- varImp(model, scale=FALSE)
plot(importance)
varimpAUC(cf1)
plot(varimpAUC(cf1))
plot(varimpAUC(cf1), scale=FALSE)
varimpAUC(cf1)
type(varimpAUC(cf1))
typeof(varimpAUC(cf1))
sort(varimpAUC(cf1))
plot(importance)
#3
install.packages("boruta")
#3
install.packages("Boruta")
library(Boruta)
boruta_output <- Boruta(weather_condition ~ ., data=weather_data_subset, doTrace=0)
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)
roughFixMod <- TentativeRoughFix(boruta_output)
boruta_signif <- getSelectedAttributes(roughFixMod)
print(boruta_signif)
imps <- attStats(roughFixMod)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2[order(-imps2$meanImp), ])
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")
#4
set.seed(17)
rPartMod <- train(weather_condition ~ ., data=weather_data_subset, method="rpart")
rpartImp <- varImp(rPartMod)
print(rpartImp)
plot(importance)
rPartMod <- train(weather_condition ~ ., data=weather_data_subset, method="RRF")
rPartMod <- train(weather_condition ~ ., data=weather_data_subset, method="RRF")
#5
set.seed(17)
rPartMod <- train(weather_condition ~ ., data=weather_data_subset, method="lasso")
rPartMod <- train(weather_condition ~ ., data=weather_data_subset, method="ada")
weather_data_subset[weather_condition ~ .]
weather_data_subset[,-1]
weather_data_subset[,-21]
weather_data_subset[,-20]
weather_data_subset[21,]
weather_data_subset[21]
#5
fisher_score = do.fscore(weather_data_subset[,-21], weather_data_subset[21])
#5
install.packages("Rdimtools")
library(Rdimtools)
fisher_score = do.fscore(weather_data_subset[,-21], weather_data_subset[21])
typeof(weather_data_subset[21])
typeof(weather_data_subset[,-21])
weather_data_subset_features <- weather_data_subset[,-21]
weather_data_subset_lables <- weather_data_subset[21]
fisher_score = do.fscore(weather_data_subset_features, weather_data_subset_lables)
c(weather_data_subset$weather_condition)
fisher_score = do.fscore(weather_data_subset_features, c(weather_data_subset$weather_condition))
#5
install.packages("FSelector")
library(FSelector)
#5
weights <- information.gain(weather_condition~., weather_data_subset)
